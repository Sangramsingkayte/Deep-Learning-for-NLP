Natural Language Processing Practicing 

This work builds on the outstanding work which exists on Natural Language Processing. These range from classics like Jurafsky's Speech and Language Processing to rather modern work in The Deep Learning Book by Ian Goodfellow et al.




Text Classification- Perfect for Getting Started! We learn better with code-first approaches

Text Cleaning - Covers some simple ideas like:
  - Stop words removal
  - Lemmatization

Spell Correction-covers almost everything that you will ever need to get started with spell correction, similar words problems and so on

Leveraging Linguistics-is an important toolkit in any practitioners toolkit. Using spaCy and textacy we look at two interesting challenges and how to tackle them: 
- Redacting names 
  - Named Entity Recognition
- Question and Answer Generation
  - Part of Speech Tagging
  - Dependency Parsing


Text Representations- is about converting text to numerical representations aka vectors
- Covers popular celebrities: word2vec, fasttext and doc2vec - document similarity using the same
- Programmer's Guide to gensim

Modern Methods for Text Classification- is simple, exploratory and talks about:
- Simple Classifiers and How to Optimize Them from scikit-learn
- How to combine and ensemble them for increased performance
- Builds intuition for ensembling - so that you can write your own ensembling techniques

Deep Learning for NLP- is less about fancy data modeling, and more engineering for Deep Learning
- From scratch code tutorial with Text Classification as an example
- Using PyTorch and torchtext
- Write our own data loaders, pre-processing, training loop and other utilities

Building your own Chatbot- from scratch in 30 minutes. We use this to explore unsupervised learning and put together several of the ideas we have already seen. 
- simpler, direct problem formulation instead of complicated chatbot tutorials commonly seen
- intents, responses and templates in chat bot parlance
- hacking word based similarity engine to work with little to no training samples
